{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6df36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa936e43",
   "metadata": {},
   "source": [
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Oracle_logo.svg/2560px-Oracle_logo.svg.png\" width=\"200\" align = \"left\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b30b4",
   "metadata": {},
   "source": [
    "# **<h1 align =\"right\"><b> Detect Fashion objects in image</b></h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7ca9b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cebeb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## + generating and returning an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e89ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3c786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensorflow conda\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f336f60",
   "metadata": {},
   "source": [
    "# **1. Model 1 - Yolov5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f011f86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01212526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad778e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-272-gde64179 Python-3.8.13 torch-2.1.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 476 layers, 87279442 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\"long sleeve top\",\"trousers\"]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')  # create model\n",
    "model = torch.hub.load('yolov5', 'custom', path='best.pt', source='local')\n",
    "\n",
    "model.iou = 0.40  # NMS IoU threshold (0-1)\n",
    "model.conf = 0.60  # confidence threshold (0-1)\n",
    "\n",
    "#open image\n",
    "img = Image.open(f'./images_clothing/vb_2.jpg')\n",
    "\n",
    "#apply model 1\n",
    "results = model(img, size=640)\n",
    "\n",
    "df_output = results.pandas().xyxy[0]\n",
    "output_model_1 = df_output['name'].to_json(orient='records')\n",
    "output_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cf2685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-1-11 Python-3.8.13 torch-2.1.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 720x1280 2 persons, 2 ties\n",
      "Speed: 523.4ms pre-process, 84.8ms inference, 11.5ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # or yolov5n - yolov5x6, custom\n",
    "\n",
    "# Images\n",
    "img = \"https://ultralytics.com/images/zidane.jpg\"  # or file, Path, PIL, OpenCV, numpy, list\n",
    "\n",
    "# Inference\n",
    "results = model(img)\n",
    "\n",
    "# Results\n",
    "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25376bc7",
   "metadata": {},
   "source": [
    "## **3. Create boilerplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b935f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "WARNING:py.warnings:/tmp/ipykernel_19288/2230555881.py:2: DeprecationWarning: The `ads.common.model_metadata` is deprecated in `oracle-ads 2.6.8` and will be removed in future release. Use the `ads.model.model_metadata` instead.\n",
      "  from ads.common.model_metadata import UseCaseType\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_19288/2230555881.py:3: DeprecationWarning: The `ads.common.model_artifact` is deprecated in `oracle-ads 2.6.9` and will be removed in `oracle-ads 3.0`.Use framework specific Model utility class for saving and deploying model. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  from ads.common.model_artifact import ModelArtifact\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_19288/2230555881.py:4: DeprecationWarning: The `ads.common.model_export_util` is deprecated in `oracle-ads 2.6.9` and will be removed in `oracle-ads 3.0`. Use framework specific Model utility class for saving and deploying model. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  from ads.common.model_export_util import prepare_generic_model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ads.model.framework.tensorflow_model import TensorFlowModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "138c440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !odsc conda init -b conda_environment_yolov5 -n frqap2zhtzbe -a resource_principal\n",
    "# !odsc conda publish -s tensorflow28_p38_gpu_v1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5763a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_19288/1158360790.py:6: DeprecationWarning: Method prepare_generic_model is deprecated in 2.6.6 and will be removed in a future release. Use framework specific Model utility class for saving and deploying model. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  artifact = prepare_generic_model(\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ADS:As force_overwrite is set to True, all the existing files in the ./artifacts_fashion_v1 will be removed\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "WARNING:ADS:Taxonomy metadata was not extracted. To auto-populate taxonomy metadata the model must be provided. Pass the model as a parameter to .prepare_generic_model(model=model, usecase_type=UseCaseType.REGRESSION). Alternative way is using atifact.populate_metadata(model=model, usecase_type=UseCaseType.REGRESSION).\n"
     ]
    }
   ],
   "source": [
    "#path to artifacts and conda slug\n",
    "path_to_artifacts = './artifacts_fashion_v1'\n",
    "conda_env = 'oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/gpu/TensorFlow 2.8 for GPU on Python 3.8/1.0/tensorflow28_p38_gpu_v1'   \n",
    "\n",
    "#create default artifacts\n",
    "artifact = prepare_generic_model(\n",
    "    path_to_artifacts, \n",
    "    fn_artifact_files_included=False, \n",
    "    force_overwrite=True, \n",
    "    inference_conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728810c",
   "metadata": {},
   "source": [
    "## **3. Download embedding model files locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8729fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_artifacts = './artifacts_fashion_v1'\n",
    "\n",
    "# this will download the files and store them in the model artifacts\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model.save_pretrained(path_to_artifacts)\n",
    "processor.save_pretrained(path_to_artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f58c4b",
   "metadata": {},
   "source": [
    "## **5. one script - score.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a556a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the full yolov5 repo to the artifacst\n",
    "## add the weights to the artifacst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa861451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: omitting directory â€˜./yolov5â€™\n"
     ]
    }
   ],
   "source": [
    "!cp -R ./yolov5 ./artifacts_fashion_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ead44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp best.pt ./artifacts_fashion_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d8e6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./artifacts_fashion_v1/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"{path_to_artifacts}/score.py\"\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import sys\n",
    "import glob\n",
    "import ads\n",
    "import urllib\n",
    "from yolov5 import models, utils \n",
    "import base64\n",
    "import uuid\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import json\n",
    "\n",
    "def load_model():\n",
    "    class DummyModel:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "    return DummyModel()\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "\n",
    "    print(\"Get image\")\n",
    "    #load base64 image from data\n",
    "    #get the base64 images from the payload\n",
    "    input_data = data['data']['input_image']\n",
    "    \n",
    "    #save image locally\n",
    "    #input image folder\n",
    "    path_input_image_locally = \"/home/datascience/images\" \n",
    "    \n",
    "    #delete folder when exists\n",
    "    if os.path.exists(path_input_image_locally):\n",
    "        shutil.rmtree(path_input_image_locally)\n",
    "    \n",
    "    #make as new folder\n",
    "    if not os.path.exists(path_input_image_locally):         \n",
    "        os.makedirs(path_input_image_locally)\n",
    "    \n",
    "    print(\"Decode and save image\")\n",
    "    ##### decoding of image\n",
    "    img_bytes_p = io.BytesIO(base64.b64decode(input_data.encode('utf-8')))\n",
    "    input_image = Image.open(img_bytes_p).resize((224, 224))  \n",
    "    \n",
    "    #save image locally     \n",
    "    input_image = input_image.save(path_input_image_locally + \"/img_1.jpg\")\n",
    "    \n",
    "    ####\n",
    "    #### Start Model 1\n",
    "    ####\n",
    "\n",
    "    print(\"Start Model 1 - Yolov5\")\n",
    "    #load model\n",
    "    model = torch.hub.load('yolov5', 'custom', path='best.pt', source='local')\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    model.iou = 0.40  # NMS IoU threshold (0-1)\n",
    "    model.conf = 0.60  # confidence threshold (0-1)\n",
    "\n",
    "    #open image from locally\n",
    "    img = Image.open(path_input_image_locally + \"/img_1.jpg\")\n",
    "\n",
    "    print(\"apply model to image\")\n",
    "    #apply model 1\n",
    "    results = model(img, size=640)\n",
    "\n",
    "    df_output = results.pandas().xyxy[0]\n",
    "    output_model_1 = df_output['name'].to_json(orient='records')\n",
    "    print(output_model_1)\n",
    "\n",
    "    ####\n",
    "    #### Start Model 2 - The embedding\n",
    "    ####\n",
    "    \n",
    "    print(\"Start model 2 - generating embedding\")\n",
    "#     model = CLIPModel.from_pretrained(\"./artifacts_fashion_v1\", local_files_only=True)               #in notebook\n",
    "#     processor = CLIPProcessor.from_pretrained(\"./artifacts_fashion_v1\", local_files_only=True)        #in notebook\n",
    "     \n",
    "    model = CLIPModel.from_pretrained(\"./\", local_files_only=True)                 #as model deployment\n",
    "    processor = CLIPProcessor.from_pretrained(\"./\", local_files_only=True)         #as model deployment\n",
    "    \n",
    "\n",
    "    #convert to vector (text is not relevant for now)\n",
    "    inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=img, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    image_embedding = outputs.image_embeds  #tensor\n",
    "\n",
    "    #convert tensor to numpy array\n",
    "    image_embedding_np = image_embedding[0].cpu().detach().numpy()\n",
    "\n",
    "    #fina list of embeddings for the image\n",
    "    embeddings = image_embedding_np.tolist()   #load array of vectors\n",
    "    \n",
    "    return {'prediction': {'cloting_features': output_model_1, 'embedding':embeddings}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48442fa1",
   "metadata": {},
   "source": [
    "## **Create payload image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc796cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get image\n",
      "Decode and save image\n",
      "Start Model 1 - Yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-272-gde64179 Python-3.8.13 torch-2.1.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 476 layers, 87279442 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "apply model to image\n",
      "[\"vest dress\"]\n",
      "Start model 2 - generating embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': {'cloting_features': '[\"vest dress\"]',\n",
       "  'embedding': [0.006523911375552416,\n",
       "   -0.002405452774837613,\n",
       "   0.025258051231503487,\n",
       "   0.003170371986925602,\n",
       "   -0.029542716220021248,\n",
       "   -0.04646524041891098,\n",
       "   -0.029391082003712654,\n",
       "   0.0002547792682889849,\n",
       "   0.016864126548171043,\n",
       "   0.02708260901272297,\n",
       "   -0.007925298996269703,\n",
       "   -0.00945043284446001,\n",
       "   0.07673767954111099,\n",
       "   -0.015081007964909077,\n",
       "   0.00583732919767499,\n",
       "   0.007842558436095715,\n",
       "   -0.018656523898243904,\n",
       "   -0.020165644586086273,\n",
       "   -0.02108377404510975,\n",
       "   -0.021225417032837868,\n",
       "   0.00045573635725304484,\n",
       "   -0.018145862966775894,\n",
       "   0.0493566133081913,\n",
       "   -0.0076074120588600636,\n",
       "   -0.02066146209836006,\n",
       "   0.00998665951192379,\n",
       "   -0.025039292871952057,\n",
       "   -0.00011539166735019535,\n",
       "   0.00040869126678444445,\n",
       "   -0.022142712026834488,\n",
       "   -0.024592019617557526,\n",
       "   -0.012825765646994114,\n",
       "   -0.013623610138893127,\n",
       "   -0.030496489256620407,\n",
       "   -0.019168652594089508,\n",
       "   0.02390037290751934,\n",
       "   -0.041435569524765015,\n",
       "   0.026543952524662018,\n",
       "   -0.008327631279826164,\n",
       "   0.1228625699877739,\n",
       "   -0.00421179598197341,\n",
       "   0.02199847251176834,\n",
       "   -0.006261397618800402,\n",
       "   0.03199386969208717,\n",
       "   0.004773998633027077,\n",
       "   -0.0511833056807518,\n",
       "   -0.03986518830060959,\n",
       "   0.000669485772959888,\n",
       "   -0.006365919951349497,\n",
       "   0.035948362201452255,\n",
       "   0.0175403393805027,\n",
       "   -0.013769201934337616,\n",
       "   -0.024799132719635963,\n",
       "   -0.03963020443916321,\n",
       "   -0.021509937942028046,\n",
       "   -0.011894751340150833,\n",
       "   0.044977255165576935,\n",
       "   -0.018255187198519707,\n",
       "   -0.00452321907505393,\n",
       "   0.009930234402418137,\n",
       "   -0.0524836890399456,\n",
       "   0.04511840268969536,\n",
       "   0.0062926760874688625,\n",
       "   -0.03127934783697128,\n",
       "   0.02339760772883892,\n",
       "   -0.00379768293350935,\n",
       "   -0.026043666526675224,\n",
       "   -0.01659390516579151,\n",
       "   0.0364452563226223,\n",
       "   -0.006462858058512211,\n",
       "   -0.010056820698082447,\n",
       "   0.07131314277648926,\n",
       "   0.020060207694768906,\n",
       "   -0.009870955720543861,\n",
       "   -0.034361764788627625,\n",
       "   0.01573597453534603,\n",
       "   0.0022911850828677416,\n",
       "   -0.0453210286796093,\n",
       "   0.02468148246407509,\n",
       "   -0.0034036876168102026,\n",
       "   -0.0038296885322779417,\n",
       "   0.012986259534955025,\n",
       "   0.023568442091345787,\n",
       "   0.020523371174931526,\n",
       "   0.004675561096519232,\n",
       "   0.0429011806845665,\n",
       "   -0.02368507720530033,\n",
       "   -0.03266612067818642,\n",
       "   -0.01849503628909588,\n",
       "   -0.015193767845630646,\n",
       "   0.016617419198155403,\n",
       "   -0.019205991178750992,\n",
       "   -0.6482264995574951,\n",
       "   0.02657335251569748,\n",
       "   0.021848032251000404,\n",
       "   0.008558489382266998,\n",
       "   0.03081466071307659,\n",
       "   -0.024661090224981308,\n",
       "   0.02563590742647648,\n",
       "   0.01720348931849003,\n",
       "   0.00870721135288477,\n",
       "   -0.048998672515153885,\n",
       "   0.021888939663767815,\n",
       "   0.018129320815205574,\n",
       "   0.042890515178442,\n",
       "   0.0010258726542815566,\n",
       "   0.08669103682041168,\n",
       "   -0.027813974767923355,\n",
       "   0.014009292237460613,\n",
       "   -0.017172329127788544,\n",
       "   0.002451661741361022,\n",
       "   -0.02212223969399929,\n",
       "   0.028267597779631615,\n",
       "   -0.03079667128622532,\n",
       "   0.00012811891792807728,\n",
       "   0.027827130630612373,\n",
       "   -0.049913372844457626,\n",
       "   0.01695411279797554,\n",
       "   0.04355834051966667,\n",
       "   -0.027867238968610764,\n",
       "   -0.016208646818995476,\n",
       "   0.000780308386310935,\n",
       "   0.009825685061514378,\n",
       "   -0.010865473188459873,\n",
       "   0.025906311348080635,\n",
       "   -0.011907849460840225,\n",
       "   -0.026348965242505074,\n",
       "   -0.015146838501095772,\n",
       "   0.03337499126791954,\n",
       "   -0.008918684907257557,\n",
       "   0.013522714376449585,\n",
       "   0.02567160502076149,\n",
       "   0.02429342083632946,\n",
       "   0.08076342195272446,\n",
       "   -0.028947945684194565,\n",
       "   0.011896626092493534,\n",
       "   -0.014400998130440712,\n",
       "   0.021323367953300476,\n",
       "   -0.0042151533998548985,\n",
       "   0.021458378061652184,\n",
       "   -0.012196294963359833,\n",
       "   -0.06292601674795151,\n",
       "   -0.04379524663090706,\n",
       "   0.016786018386483192,\n",
       "   -0.0035736477002501488,\n",
       "   0.037440501153469086,\n",
       "   0.010085459798574448,\n",
       "   0.012766270898282528,\n",
       "   -0.029116662219166756,\n",
       "   0.023272007703781128,\n",
       "   0.05505094304680824,\n",
       "   0.014266472309827805,\n",
       "   0.14558933675289154,\n",
       "   -0.0073111094534397125,\n",
       "   0.0014560497365891933,\n",
       "   0.01524347998201847,\n",
       "   -0.03554219380021095,\n",
       "   0.01916872151196003,\n",
       "   -0.008397838100790977,\n",
       "   0.0006376735400408506,\n",
       "   0.034430090337991714,\n",
       "   0.035920150578022,\n",
       "   -0.02598225325345993,\n",
       "   0.024815229699015617,\n",
       "   -0.023125754669308662,\n",
       "   0.008060149848461151,\n",
       "   0.018316665664315224,\n",
       "   0.0035158079117536545,\n",
       "   0.016280408948659897,\n",
       "   -0.0013590098824352026,\n",
       "   -0.013552081771194935,\n",
       "   0.008059307932853699,\n",
       "   -0.0596088245511055,\n",
       "   -0.06540355086326599,\n",
       "   -0.005213695578277111,\n",
       "   0.019408972933888435,\n",
       "   0.052077069878578186,\n",
       "   -0.0070077660493552685,\n",
       "   0.08652926236391068,\n",
       "   0.03892575949430466,\n",
       "   0.03233787417411804,\n",
       "   -0.040952447801828384,\n",
       "   0.025646144524216652,\n",
       "   0.012724153697490692,\n",
       "   -0.0459786094725132,\n",
       "   0.005968387704342604,\n",
       "   -0.017368093132972717,\n",
       "   -0.025406284257769585,\n",
       "   0.02530619129538536,\n",
       "   8.319003973156214e-05,\n",
       "   0.013296539895236492,\n",
       "   -0.01137376856058836,\n",
       "   -0.017539141699671745,\n",
       "   -0.054248251020908356,\n",
       "   -0.016064459457993507,\n",
       "   -0.0034517445601522923,\n",
       "   0.015173721127212048,\n",
       "   0.040026478469371796,\n",
       "   0.05793383717536926,\n",
       "   -0.0003123002825304866,\n",
       "   -0.009040385484695435,\n",
       "   0.020010465756058693,\n",
       "   0.013422181829810143,\n",
       "   0.020326310768723488,\n",
       "   0.009437033906579018,\n",
       "   -0.0198505911976099,\n",
       "   -0.022258935496211052,\n",
       "   -0.03172167390584946,\n",
       "   0.03894973173737526,\n",
       "   0.008891060017049313,\n",
       "   0.06217757984995842,\n",
       "   0.023080136626958847,\n",
       "   -0.0029301501344889402,\n",
       "   -0.01544089987874031,\n",
       "   -0.049245283007621765,\n",
       "   0.012010948732495308,\n",
       "   -0.010961315594613552,\n",
       "   0.03509513661265373,\n",
       "   0.036708489060401917,\n",
       "   -0.06257177889347076,\n",
       "   0.032727327197790146,\n",
       "   -0.01945250667631626,\n",
       "   0.0021463329903781414,\n",
       "   0.01979820989072323,\n",
       "   -0.03199736773967743,\n",
       "   0.03177966922521591,\n",
       "   0.030139131471514702,\n",
       "   0.0027362992987036705,\n",
       "   -0.026332039386034012,\n",
       "   0.025545893236994743,\n",
       "   0.05445655435323715,\n",
       "   0.00036129585350863636,\n",
       "   0.014383742585778236,\n",
       "   -0.0017675458220764995,\n",
       "   -0.013541536405682564,\n",
       "   -0.03873986005783081,\n",
       "   0.014012462459504604,\n",
       "   0.009303798899054527,\n",
       "   -0.024490533396601677,\n",
       "   0.02352995052933693,\n",
       "   0.008903243578970432,\n",
       "   -0.033735182136297226,\n",
       "   -0.02578885480761528,\n",
       "   -0.010740370489656925,\n",
       "   0.014684277586638927,\n",
       "   0.03695797920227051,\n",
       "   -0.03726419061422348,\n",
       "   0.017532221972942352,\n",
       "   -0.05107715353369713,\n",
       "   0.012708144262433052,\n",
       "   -0.03712385147809982,\n",
       "   0.0011300425976514816,\n",
       "   0.01572309248149395,\n",
       "   -0.03512981906533241,\n",
       "   0.01732960157096386,\n",
       "   0.008838733658194542,\n",
       "   -0.028819147497415543,\n",
       "   -0.007040766533464193,\n",
       "   0.05787479132413864,\n",
       "   0.021024366840720177,\n",
       "   -0.017750650644302368,\n",
       "   -0.015873467549681664,\n",
       "   -0.018811793997883797,\n",
       "   0.13463865220546722,\n",
       "   -0.0369148850440979,\n",
       "   -0.014183170162141323,\n",
       "   -0.012829264625906944,\n",
       "   0.007725879084318876,\n",
       "   0.012912693433463573,\n",
       "   -0.016667665913701057,\n",
       "   -0.007986350916326046,\n",
       "   -0.01657259650528431,\n",
       "   -0.014418678358197212,\n",
       "   -0.013856747187674046,\n",
       "   -0.045621465891599655,\n",
       "   0.025333844125270844,\n",
       "   0.01608946919441223,\n",
       "   -0.040216680616140366,\n",
       "   -0.022586645558476448,\n",
       "   -0.017186705023050308,\n",
       "   0.003735422855243087,\n",
       "   0.0024859108962118626,\n",
       "   -0.007899662479758263,\n",
       "   0.03371415287256241,\n",
       "   -0.01172877848148346,\n",
       "   -0.025351759046316147,\n",
       "   -0.058982305228710175,\n",
       "   -0.012466900050640106,\n",
       "   -0.037229638546705246,\n",
       "   -0.015410377644002438,\n",
       "   0.021169491112232208,\n",
       "   0.04402899369597435,\n",
       "   0.010273204185068607,\n",
       "   -0.035508666187524796,\n",
       "   0.018450237810611725,\n",
       "   0.016966963186860085,\n",
       "   -0.04892412945628166,\n",
       "   -0.05526823177933693,\n",
       "   0.05739906430244446,\n",
       "   -0.013300090096890926,\n",
       "   0.002474099863320589,\n",
       "   -0.0038540884852409363,\n",
       "   -0.0072333598509430885,\n",
       "   -0.031435705721378326,\n",
       "   0.0009285769192501903,\n",
       "   0.05678451061248779,\n",
       "   -0.0037167533300817013,\n",
       "   0.04019715636968613,\n",
       "   0.0055687204003334045,\n",
       "   0.03364889696240425,\n",
       "   -0.03669237345457077,\n",
       "   -0.015556157566606998,\n",
       "   0.08077026903629303,\n",
       "   0.041799500584602356,\n",
       "   0.004494221415370703,\n",
       "   0.01687336154282093,\n",
       "   0.038590870797634125,\n",
       "   0.06750146299600601,\n",
       "   0.025813719257712364,\n",
       "   -0.023221012204885483,\n",
       "   0.016922609880566597,\n",
       "   0.10658947378396988,\n",
       "   0.003754102624952793,\n",
       "   -0.03382646664977074,\n",
       "   0.03445645421743393,\n",
       "   0.023205813020467758,\n",
       "   -0.023456508293747902,\n",
       "   0.04798183590173721,\n",
       "   0.0027858817484229803,\n",
       "   0.0016992907039821148,\n",
       "   0.04712685942649841,\n",
       "   -0.0313655324280262,\n",
       "   0.0027557911816984415,\n",
       "   -0.041162051260471344,\n",
       "   -0.02117285318672657,\n",
       "   -0.009035468101501465,\n",
       "   0.009082688018679619,\n",
       "   0.008444155566394329,\n",
       "   -0.026645399630069733,\n",
       "   -0.031832505017519,\n",
       "   0.05114632472395897,\n",
       "   -0.014335084706544876,\n",
       "   -0.05224498733878136,\n",
       "   -0.011646246537566185,\n",
       "   -0.01623152382671833,\n",
       "   0.0058546606451272964,\n",
       "   -0.0008146294858306646,\n",
       "   0.02237795665860176,\n",
       "   -0.02775072306394577,\n",
       "   0.018740151077508926,\n",
       "   -0.012386975809931755,\n",
       "   -0.006178931333124638,\n",
       "   0.01846528798341751,\n",
       "   0.021711600944399834,\n",
       "   0.002838119864463806,\n",
       "   -0.07764240354299545,\n",
       "   -0.019528066739439964,\n",
       "   0.1226712316274643,\n",
       "   0.04488087445497513,\n",
       "   0.0008950678166002035,\n",
       "   0.037238460034132004,\n",
       "   0.007621363271027803,\n",
       "   -0.01218925230205059,\n",
       "   -0.0023501247633248568,\n",
       "   -0.003280854783952236,\n",
       "   -0.021633055061101913,\n",
       "   -0.000480046117445454,\n",
       "   0.04517688602209091,\n",
       "   -0.0051394300535321236,\n",
       "   0.017974479123950005,\n",
       "   -0.02272404357790947,\n",
       "   -0.016239486634731293,\n",
       "   0.011247551068663597,\n",
       "   -0.020851971581578255,\n",
       "   0.007364329416304827,\n",
       "   -0.04286640137434006,\n",
       "   0.1464911699295044,\n",
       "   0.001747015630826354,\n",
       "   0.09038715809583664,\n",
       "   0.04397061467170715,\n",
       "   -0.0175924114882946,\n",
       "   0.025961846113204956,\n",
       "   0.028526291251182556,\n",
       "   -0.08690337091684341,\n",
       "   -0.013797980733215809,\n",
       "   0.02501259371638298,\n",
       "   0.016629405319690704,\n",
       "   -0.02970629371702671,\n",
       "   -0.011343197897076607,\n",
       "   0.07315711677074432,\n",
       "   -0.03440641611814499,\n",
       "   -0.07912443578243256,\n",
       "   -0.04892124980688095,\n",
       "   0.022827742621302605,\n",
       "   -0.03648018836975098,\n",
       "   -0.023945964872837067,\n",
       "   0.03519870713353157,\n",
       "   -0.028599873185157776,\n",
       "   0.026544686406850815,\n",
       "   -0.057378727942705154,\n",
       "   0.010327176190912724,\n",
       "   -0.02561088465154171,\n",
       "   0.0313422828912735,\n",
       "   -0.024180641397833824,\n",
       "   0.0012972453841939569,\n",
       "   0.0021170119289308786,\n",
       "   0.024509690701961517,\n",
       "   -0.0038959437515586615,\n",
       "   0.006651477888226509,\n",
       "   -0.008380343206226826,\n",
       "   -0.055227018892765045,\n",
       "   -0.024896102026104927,\n",
       "   0.0011076295049861073,\n",
       "   -0.04926703870296478,\n",
       "   -0.03992420434951782,\n",
       "   -0.0011294038267806172,\n",
       "   0.008784831501543522,\n",
       "   -0.022513587027788162,\n",
       "   0.026818564161658287,\n",
       "   -0.051495056599378586,\n",
       "   0.015319349244236946,\n",
       "   0.021438250318169594,\n",
       "   7.666558303753845e-06,\n",
       "   0.011865257285535336,\n",
       "   -0.0066333976574242115,\n",
       "   -0.004829719662666321,\n",
       "   0.004944233689457178,\n",
       "   -0.04699840769171715,\n",
       "   -0.24569813907146454,\n",
       "   -0.0078101083636283875,\n",
       "   0.04636716842651367,\n",
       "   -0.0033819512464106083,\n",
       "   0.0678795799612999,\n",
       "   -0.003697781590744853,\n",
       "   0.003381921211257577,\n",
       "   -0.0006789130275137722,\n",
       "   0.0311695896089077,\n",
       "   -0.017856035381555557,\n",
       "   -0.0042345840483903885,\n",
       "   -0.029977861791849136,\n",
       "   -0.011533503420650959,\n",
       "   0.0174862127751112,\n",
       "   -0.03306524083018303,\n",
       "   0.017730075865983963,\n",
       "   -0.021007858216762543,\n",
       "   -0.018748752772808075,\n",
       "   0.026897652074694633,\n",
       "   0.00028549108537845314,\n",
       "   -0.004529147874563932,\n",
       "   -0.034714069217443466,\n",
       "   -0.05298720672726631,\n",
       "   -0.02296510525047779,\n",
       "   0.00546285742893815,\n",
       "   0.08500541001558304,\n",
       "   -0.050529222935438156,\n",
       "   0.029697272926568985,\n",
       "   0.0033788688015192747,\n",
       "   -0.004760698415338993,\n",
       "   -0.021862637251615524,\n",
       "   -0.04689132794737816,\n",
       "   -0.0149246696382761,\n",
       "   -0.020603278651833534,\n",
       "   0.016423260793089867,\n",
       "   -0.02657029964029789,\n",
       "   0.040464915335178375,\n",
       "   0.011552025564014912,\n",
       "   -0.0017562045250087976,\n",
       "   0.007071740925312042,\n",
       "   0.003300282172858715,\n",
       "   0.009276419878005981,\n",
       "   -0.03523124381899834,\n",
       "   -0.007471648044884205,\n",
       "   -0.022571705281734467,\n",
       "   0.03332534804940224,\n",
       "   0.05087035521864891,\n",
       "   -0.025864966213703156,\n",
       "   -0.01635129563510418,\n",
       "   -0.02003473974764347,\n",
       "   0.029455535113811493,\n",
       "   -0.00031474302522838116,\n",
       "   0.0010193452471867204,\n",
       "   -0.034466009587049484,\n",
       "   0.025411996990442276,\n",
       "   -0.020692402496933937,\n",
       "   -0.044545672833919525,\n",
       "   -0.000583206070587039,\n",
       "   -0.0029311387334018946,\n",
       "   -0.0010606886353343725,\n",
       "   -0.006578288972377777,\n",
       "   0.015528639778494835,\n",
       "   -0.02038709446787834,\n",
       "   0.011389839462935925,\n",
       "   0.05545100197196007,\n",
       "   -0.016136178746819496,\n",
       "   -0.013235652819275856,\n",
       "   0.03483549505472183,\n",
       "   -0.013192999176681042,\n",
       "   -0.01326869148761034,\n",
       "   -0.003835756331682205,\n",
       "   -0.008346966467797756,\n",
       "   0.01406622864305973,\n",
       "   0.022286437451839447,\n",
       "   -0.0062754214741289616,\n",
       "   -0.009748025797307491,\n",
       "   0.040654655545949936,\n",
       "   -0.016131168231368065,\n",
       "   0.019989315420389175,\n",
       "   0.02682880125939846,\n",
       "   -0.013717696070671082,\n",
       "   0.005580672528594732,\n",
       "   0.04819639027118683,\n",
       "   -0.007078180555254221]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path=\"./img_6.jpg\"\n",
    "\n",
    "payload_string = str()\n",
    "full_payload_string = str()\n",
    "    \n",
    "with open(input_path, \"rb\") as image2string:\n",
    "    converted_string = base64.b64encode(image2string.read()).decode('ascii')\n",
    "               \n",
    "payload1 = json.dumps(converted_string)\n",
    "json_payload1 = json.loads(payload1)\n",
    "            \n",
    "payload_json = {'data':{'input_image': json_payload1}}\n",
    "\n",
    "###################################\n",
    "# try function\n",
    "\n",
    "predict(payload_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f066a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./artifacts_fashion_v1/runtime.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"{path_to_artifacts}/runtime.yaml\"\n",
    "\n",
    "# Model runtime environment\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/gpu/TensorFlow 2.8 for GPU on Python 3.8/1.0/tensorflow28_p38_gpu_v1\n",
    "    INFERENCE_ENV_SLUG: tensorflow28_p38_gpu_v1\n",
    "    INFERENCE_ENV_TYPE: published\n",
    "    INFERENCE_PYTHON_VERSION: '3.8'\n",
    "MODEL_PROVENANCE:\n",
    "  PROJECT_OCID: ocid1.datascienceproject.oc1.eu-frankfurt-1.amaaaaaangencdyaik5ssdqk4as2bhldxprh7vnqpk7yycsm7vymd344cgua\n",
    "  TENANCY_OCID: ocid1.tenancy.oc1..aaaaaaaabu5fgingcjq3vc7djuwsdcutdxs4gsws6h4kfoldqpjuggxprgoa\n",
    "  TRAINING_COMPARTMENT_OCID: ocid1.compartment.oc1..aaaaaaaae3n6r6hrjipbap2hojicrsvkzatrtlwvsyrpyjd7wjnw4za3m75q\n",
    "  TRAINING_CONDA_ENV:\n",
    "    TRAINING_ENV_PATH: oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/gpu/TensorFlow 2.8 for GPU on Python 3.8/1.0/tensorflow28_p38_gpu_v1\n",
    "    TRAINING_ENV_SLUG: tensorflow28_p38_gpu_v1\n",
    "    TRAINING_ENV_TYPE: published\n",
    "    TRAINING_PYTHON_VERSION: '3.8'\n",
    "  TRAINING_REGION: eu-frankfurt-1\n",
    "  TRAINING_RESOURCE_OCID: ocid1.datasciencenotebooksession.oc1.eu-frankfurt-1.amaaaaaangencdyacxmsz5ycch762wjc54udhibtl3m4nacuaf7shrvyoktq\n",
    "  USER_OCID: ocid1.saml2idp.oc1..aaaaaaaar3ydw5hoiob7dfjzoom2dvbhqkkd5fat6m7upe72emlsxhsfrbfa/bob.peulen@oracle.com\n",
    "  VM_IMAGE_INTERNAL_ID: NB1480-DCGPU131-VMP64-VMA1585-BI681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb04044",
   "metadata": {},
   "source": [
    "## **Check artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5d6cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['config.json', '.ipynb_checkpoints', 'merges.txt', 'yolov5', 'special_tokens_map.json', 'score.py', 'models--openai--clip-vit-base-patch32', 'runtime.yaml', 'test_json_output.json', 'best.pt', 'tokenizer_config.json', 'preprocessor_config.json', 'vocab.json', 'pytorch_model.bin', 'tokenizer.json']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test key</th>\n",
       "      <th>Test name</th>\n",
       "      <th>Result</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runtime_env_path</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runtime_env_python</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runtime_path_exist</td>\n",
       "      <td>Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runtime_version</td>\n",
       "      <td>Check that field MODEL_ARTIFACT_VERSION is set to 3.0</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runtime_yaml</td>\n",
       "      <td>Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>score_load_model</td>\n",
       "      <td>Check that load_model() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>score_predict</td>\n",
       "      <td>Check that predict() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score_predict_arg</td>\n",
       "      <td>Check that all other arguments in predict() are optional and have default values</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>score_predict_data</td>\n",
       "      <td>Check that the only required argument for predict() is named \"data\"</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>score_py</td>\n",
       "      <td>Check that the file \"score.py\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>score_syntax</td>\n",
       "      <td>Check for Python syntax errors</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test key  \\\n",
       "0     runtime_env_path   \n",
       "1   runtime_env_python   \n",
       "2   runtime_path_exist   \n",
       "3      runtime_version   \n",
       "4         runtime_yaml   \n",
       "5     score_load_model   \n",
       "6        score_predict   \n",
       "7    score_predict_arg   \n",
       "8   score_predict_data   \n",
       "9             score_py   \n",
       "10        score_syntax   \n",
       "\n",
       "                                                                                                Test name  \\\n",
       "0                                             Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set   \n",
       "1           Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher   \n",
       "2                             Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.   \n",
       "3                                                   Check that field MODEL_ARTIFACT_VERSION is set to 3.0   \n",
       "4   Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory   \n",
       "5                                                                      Check that load_model() is defined   \n",
       "6                                                                         Check that predict() is defined   \n",
       "7                        Check that all other arguments in predict() are optional and have default values   \n",
       "8                                     Check that the only required argument for predict() is named \"data\"   \n",
       "9       Check that the file \"score.py\" exists and is in the top level directory of the artifact directory   \n",
       "10                                                                         Check for Python syntax errors   \n",
       "\n",
       "    Result Message  \n",
       "0   Passed          \n",
       "1   Passed          \n",
       "2   Passed          \n",
       "3   Passed          \n",
       "4   Passed          \n",
       "5   Passed          \n",
       "6   Passed          \n",
       "7   Passed          \n",
       "8   Passed          \n",
       "9   Passed          \n",
       "10  Passed          "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all should be passed\n",
    "artifact.introspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231e103d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_19288/3939112751.py:2: DeprecationWarning: Method save is deprecated in 2.6.6 and will be removed in a future release. Use framework specific Model utility class for saving and deploying model. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  catalog_entry = artifact.save(display_name='clothing_detection_and_embeddings_v1', description='clothing_detection_and_embeddings_v1', timeout=600)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ocid1.datasciencemodel.oc1.eu-frankfurt-1.amaaaaaangencdyae23kmm47pcpdpg7gplgcdxttencp6m6ldije7kfwgsiq'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model artifact to the model catalog. \n",
    "catalog_entry = artifact.save(display_name='clothing_detection_and_embeddings_v1', description='clothing_detection_and_embeddings_v1', timeout=600)\n",
    "catalog_entry.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f02bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff17d3",
   "metadata": {},
   "source": [
    "# **8. Deploy the ML Model and test real-time inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdf26f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c91ba5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdya4k5yytlulz2kvgvmjmo3qz3mmk4ed6zld5ksdi7upstq/predict\n"
     ]
    }
   ],
   "source": [
    "uri = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdya4k5yytlulz2kvgvmjmo3qz3mmk4ed6zld5ksdi7upstq/predict\"\n",
    "print(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d813a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'prediction': {'cloting_features': '[\"vest dress\"]'}}\n"
     ]
    }
   ],
   "source": [
    "config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])\n",
    "\n",
    "import json\n",
    "\n",
    "#POST request to the model\n",
    "response = requests.post(uri, json=payload_json, auth=auth)\n",
    "print(response)\n",
    "xx = (json.loads(response.content))\n",
    "print(xx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf51aeb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279d7b9",
   "metadata": {},
   "source": [
    "## **Local UI for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "441038c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "import json\n",
    "\n",
    "config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])\n",
    "\n",
    "\n",
    "\n",
    "def full_function(input_image):\n",
    "    \n",
    "    input_path=input_image #is the path\n",
    "\n",
    "    payload_string = str()\n",
    "    full_payload_string = str()\n",
    "    \n",
    "    with open(input_path, \"rb\") as image2string:\n",
    "        converted_string = base64.b64encode(image2string.read()).decode('ascii')\n",
    "\n",
    "    payload1 = json.dumps(converted_string)\n",
    "    json_payload1 = json.loads(payload1)\n",
    "\n",
    "    payload_json = {'data':{'input_image': json_payload1}}\n",
    "\n",
    "    uri = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdya4k5yytlulz2kvgvmjmo3qz3mmk4ed6zld5ksdi7upstq/predict\"\n",
    "    \n",
    "    #POST request to the model\n",
    "    response = requests.post(uri, json=payload_json, auth=auth)\n",
    "    print(response)\n",
    "    xx = (json.loads(response.content))    \n",
    "\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65caf73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://4a39dadd65c17de38c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4a39dadd65c17de38c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/gradio/blocks.py\", line 2014, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_22432/2999098039.py\", line 15, in <cell line: 15>\n",
      "    gr.Interface(fn=full_function, inputs=input_image, outputs=xx, title=desc).launch(share=True, debug=True) #\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/gradio/blocks.py\", line 1926, in launch\n",
      "    self.block_thread()\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/gradio/blocks.py\", line 2017, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/gradio/networking.py\", line 43, in close\n",
      "    self.thread.join()\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "desc = \"Clothing Detection\"\n",
    "\n",
    "with gr.Blocks() as demo: \n",
    "     \n",
    "    input_image = gr.Image(label=\"source_city\", type='filepath')\n",
    "\n",
    "    xx = gr.Text(label='Detected clothes')\n",
    "\n",
    "\n",
    "    submit_btn = gr.Button(\"Run Analysis\")\n",
    "\n",
    "\n",
    "gr.Interface(fn=full_function, inputs=input_image, outputs=xx, title=desc).launch(share=True, debug=True) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a547b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e038c5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61459d28",
   "metadata": {},
   "source": [
    "## **2. Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ca105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "\n",
    "\n",
    "### Functions and labels\n",
    "###\n",
    "\n",
    "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']\n",
    "\n",
    "def fix_channels(t):\n",
    "    \"\"\"\n",
    "    Some images may have 4 channels (transparent images) or just 1 channel (black and white images), in order to let the images have only 3 channels. I am going to remove the fourth channel in transparent images and stack the single channel in back and white images.\n",
    "    :param t: Tensor-like image\n",
    "    :return: Tensor-like image with three channels\n",
    "    \"\"\"\n",
    "    if len(t.shape) == 2:\n",
    "        return ToPILImage()(torch.stack([t for i in (0, 0, 0)]))\n",
    "    if t.shape[0] == 4:\n",
    "        return ToPILImage()(t[:3])\n",
    "    if t.shape[0] == 1:\n",
    "        return ToPILImage()(torch.stack([t[0] for i in (0, 0, 0)]))\n",
    "    return ToPILImage()(t)\n",
    "    \n",
    "def idx_to_text(i):\n",
    "    return cats[i]\n",
    "\n",
    "# Random colors used for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        ax.text(xmin, ymin, idx_to_text(cl), fontsize=10,\n",
    "                bbox=dict(facecolor=c, alpha=0.8))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.savefig(\"image.png\")\n",
    "    \n",
    "    \n",
    "def visualize_predictions(image, outputs, threshold=0.8):\n",
    "    # keep only predictions with confidence >= threshold\n",
    "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > threshold\n",
    "\n",
    "    # convert predicted boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
    "\n",
    "    # plot results\n",
    "    plot_results(image, probas[keep], bboxes_scaled)\n",
    "\n",
    "    \n",
    "#############################\n",
    "\n",
    "#define model\n",
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#load iamge\n",
    "image = Image.open(open('./img_1.jpg', \"rb\"))\n",
    "image = fix_channels(ToTensor()(image))\n",
    "image = image.resize((600, 800))\n",
    "\n",
    "#apply model\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

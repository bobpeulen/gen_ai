{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875edad9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b9638",
   "metadata": {},
   "source": [
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Oracle_logo.svg/2560px-Oracle_logo.svg.png\" width=\"200\" align = \"left\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb48113",
   "metadata": {},
   "source": [
    "# **<h1 align =\"right\"><b> Detect Fashion objects in image</b></h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38143541",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfecb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensorflow conda\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362dbbe",
   "metadata": {},
   "source": [
    "# **1. Model 1 - Yolov5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5af2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d53929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_22432/501806733.py\", line 5, in <cell line: 5>\n",
      "    model = torch.load('yolov5', 'custom', path='best.pt', source='local')\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/torch/serialization.py\", line 986, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/torch/serialization.py\", line 435, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/torch/serialization.py\", line 416, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "IsADirectoryError: [Errno 21] Is a directory: 'yolov5'\n",
      "IsADirectoryError: [Errno 21] Is a directory: 'yolov5'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')  # create model\n",
    "model = torch.load('yolov5', 'custom', path='best.pt', source='local')\n",
    "\n",
    "model.iou = 0.40  # NMS IoU threshold (0-1)\n",
    "model.conf = 0.60  # confidence threshold (0-1)\n",
    "\n",
    "#open image\n",
    "img = Image.open(f'./images_clothing/img_3.jpg')\n",
    "\n",
    "#apply model 1\n",
    "results = model(img, size=640)\n",
    "\n",
    "df_output = results.pandas().xyxy[0]\n",
    "output_model_1 = df_output['name'].to_json(orient='records')\n",
    "output_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee23fa",
   "metadata": {},
   "source": [
    "## **3. Create boilerplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8cf643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_22432/2230555881.py:2: DeprecationWarning: The `ads.common.model_metadata` is deprecated in `oracle-ads 2.6.8` and will be removed in future release. Use the `ads.model.model_metadata` instead.\n",
      "  from ads.common.model_metadata import UseCaseType\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_22432/2230555881.py:3: DeprecationWarning: The `ads.common.model_artifact` is deprecated in `oracle-ads 2.6.9` and will be removed in `oracle-ads 3.0`.Use framework specific Model utility class for saving and deploying model. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  from ads.common.model_artifact import ModelArtifact\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_22432/2230555881.py:4: DeprecationWarning: The `ads.common.model_export_util` is deprecated in `oracle-ads 2.6.9` and will be removed in `oracle-ads 3.0`. Use framework specific Model utility class for saving and deploying model. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  from ads.common.model_export_util import prepare_generic_model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ads.model.framework.tensorflow_model import TensorFlowModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a42b8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !odsc conda init -b conda_environment_yolov5 -n frqap2zhtzbe -a resource_principal\n",
    "# !odsc conda publish -s tensorflow28_p38_gpu_v1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac02b405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_22432/1158360790.py:6: DeprecationWarning: Method prepare_generic_model is deprecated in 2.6.6 and will be removed in a future release. Use framework specific Model utility class for saving and deploying model. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  artifact = prepare_generic_model(\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ADS:As force_overwrite is set to True, all the existing files in the ./artifacts_fashion_v1 will be removed\n",
      "WARNING:ADS:Taxonomy metadata was not extracted. To auto-populate taxonomy metadata the model must be provided. Pass the model as a parameter to .prepare_generic_model(model=model, usecase_type=UseCaseType.REGRESSION). Alternative way is using atifact.populate_metadata(model=model, usecase_type=UseCaseType.REGRESSION).\n"
     ]
    }
   ],
   "source": [
    "#path to artifacts and conda slug\n",
    "path_to_artifacts = './artifacts_fashion_v1'\n",
    "conda_env = 'oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/gpu/TensorFlow 2.8 for GPU on Python 3.8/1.0/tensorflow28_p38_gpu_v1'   \n",
    "\n",
    "#create default artifacts\n",
    "artifact = prepare_generic_model(\n",
    "    path_to_artifacts, \n",
    "    fn_artifact_files_included=False, \n",
    "    force_overwrite=True, \n",
    "    inference_conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd425851",
   "metadata": {},
   "source": [
    "## **4. one script - score.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the full yolov5 repo to the artifacst\n",
    "## add the weights to the artifacst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475db0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./yolov5 ./artifacts_fashion_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed440daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp best.pt ./artifacts_fashion_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf694007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./artifacts_fashion_v1/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"{path_to_artifacts}/score.py\"\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import sys\n",
    "import glob\n",
    "import ads\n",
    "import urllib\n",
    "from yolov5 import models, utils \n",
    "import base64\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "def load_model():\n",
    "    class DummyModel:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "    return DummyModel()\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "\n",
    "    print(\"Get image\")\n",
    "    #load base64 image from data\n",
    "    #get the base64 images from the payload\n",
    "    input_data = data['data']['input_image']\n",
    "    \n",
    "    #save image locally\n",
    "    #input image folder\n",
    "    path_input_image_locally = \"/home/datascience/images\" \n",
    "    \n",
    "    #delete folder when exists\n",
    "    if os.path.exists(path_input_image_locally):\n",
    "        shutil.rmtree(path_input_image_locally)\n",
    "    \n",
    "    #make as new folder\n",
    "    if not os.path.exists(path_input_image_locally):         \n",
    "        os.makedirs(path_input_image_locally)\n",
    "    \n",
    "    print(\"Decode and save image\")\n",
    "    ##### decoding of image\n",
    "    img_bytes_p = io.BytesIO(base64.b64decode(input_data.encode('utf-8')))\n",
    "    input_image = Image.open(img_bytes_p).resize((224, 224))  \n",
    "    \n",
    "    #save image locally     \n",
    "    input_image = input_image.save(path_input_image_locally + \"/img_1.jpg\")\n",
    "    \n",
    "    ####\n",
    "    #### Start Model 1\n",
    "    ####\n",
    "\n",
    "    print(\"Start Model 1 - Yolov5\")\n",
    "    #load model\n",
    "    model = torch.hub.load('yolov5', 'custom', path='best.pt', source='local')\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    model.iou = 0.40  # NMS IoU threshold (0-1)\n",
    "    model.conf = 0.60  # confidence threshold (0-1)\n",
    "\n",
    "    #open image from locally\n",
    "    img = Image.open(path_input_image_locally + \"/img_1.jpg\")\n",
    "\n",
    "    print(\"apply model to image\")\n",
    "    #apply model 1\n",
    "    results = model(img, size=640)\n",
    "\n",
    "    df_output = results.pandas().xyxy[0]\n",
    "    output_model_1 = df_output['name'].to_json(orient='records')\n",
    "    print(output_model_1)\n",
    "\n",
    "    \n",
    "    return {'prediction': {'cloting_features': output_model_1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bdc75",
   "metadata": {},
   "source": [
    "## **Create payload image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdfe080b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-272-gde64179 Python-3.8.13 torch-2.1.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get image\n",
      "Decode and save image\n",
      "Start Model 1 - Yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary: 476 layers, 87279442 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply model to image\n",
      "[\"vest dress\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': {'cloting_features': '[\"vest dress\"]'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path=\"./img_6.jpg\"\n",
    "\n",
    "payload_string = str()\n",
    "full_payload_string = str()\n",
    "    \n",
    "with open(input_path, \"rb\") as image2string:\n",
    "    converted_string = base64.b64encode(image2string.read()).decode('ascii')\n",
    "               \n",
    "payload1 = json.dumps(converted_string)\n",
    "json_payload1 = json.loads(payload1)\n",
    "            \n",
    "payload_json = {'data':{'input_image': json_payload1}}\n",
    "\n",
    "###################################\n",
    "# try function\n",
    "\n",
    "predict(payload_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d39d84d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./artifacts_fashion_v1/runtime.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"{path_to_artifacts}/runtime.yaml\"\n",
    "\n",
    "# Model runtime environment\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/gpu/TensorFlow 2.8 for GPU on Python 3.8/1.0/tensorflow28_p38_gpu_v1\n",
    "    INFERENCE_ENV_SLUG: tensorflow28_p38_gpu_v1\n",
    "    INFERENCE_ENV_TYPE: published\n",
    "    INFERENCE_PYTHON_VERSION: '3.8'\n",
    "MODEL_PROVENANCE:\n",
    "  PROJECT_OCID: ocid1.datascienceproject.oc1.eu-frankfurt-1.amaaaaaangencdyaik5ssdqk4as2bhldxprh7vnqpk7yycsm7vymd344cgua\n",
    "  TENANCY_OCID: ocid1.tenancy.oc1..aaaaaaaabu5fgingcjq3vc7djuwsdcutdxs4gsws6h4kfoldqpjuggxprgoa\n",
    "  TRAINING_COMPARTMENT_OCID: ocid1.compartment.oc1..aaaaaaaae3n6r6hrjipbap2hojicrsvkzatrtlwvsyrpyjd7wjnw4za3m75q\n",
    "  TRAINING_CONDA_ENV:\n",
    "    TRAINING_ENV_PATH: oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/gpu/TensorFlow 2.8 for GPU on Python 3.8/1.0/tensorflow28_p38_gpu_v1\n",
    "    TRAINING_ENV_SLUG: tensorflow28_p38_gpu_v1\n",
    "    TRAINING_ENV_TYPE: published\n",
    "    TRAINING_PYTHON_VERSION: '3.8'\n",
    "  TRAINING_REGION: eu-frankfurt-1\n",
    "  TRAINING_RESOURCE_OCID: ocid1.datasciencenotebooksession.oc1.eu-frankfurt-1.amaaaaaangencdyacxmsz5ycch762wjc54udhibtl3m4nacuaf7shrvyoktq\n",
    "  USER_OCID: ocid1.saml2idp.oc1..aaaaaaaar3ydw5hoiob7dfjzoom2dvbhqkkd5fat6m7upe72emlsxhsfrbfa/bob.peulen@oracle.com\n",
    "  VM_IMAGE_INTERNAL_ID: NB1480-DCGPU131-VMP64-VMA1585-BI681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed4c2a",
   "metadata": {},
   "source": [
    "## **Check artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3f796ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'yolov5', 'score.py', 'runtime.yaml', 'test_json_output.json', 'best.pt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test key</th>\n",
       "      <th>Test name</th>\n",
       "      <th>Result</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runtime_env_path</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runtime_env_python</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runtime_path_exist</td>\n",
       "      <td>Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runtime_version</td>\n",
       "      <td>Check that field MODEL_ARTIFACT_VERSION is set to 3.0</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runtime_yaml</td>\n",
       "      <td>Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>score_load_model</td>\n",
       "      <td>Check that load_model() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>score_predict</td>\n",
       "      <td>Check that predict() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score_predict_arg</td>\n",
       "      <td>Check that all other arguments in predict() are optional and have default values</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>score_predict_data</td>\n",
       "      <td>Check that the only required argument for predict() is named \"data\"</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>score_py</td>\n",
       "      <td>Check that the file \"score.py\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>score_syntax</td>\n",
       "      <td>Check for Python syntax errors</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test key  \\\n",
       "0     runtime_env_path   \n",
       "1   runtime_env_python   \n",
       "2   runtime_path_exist   \n",
       "3      runtime_version   \n",
       "4         runtime_yaml   \n",
       "5     score_load_model   \n",
       "6        score_predict   \n",
       "7    score_predict_arg   \n",
       "8   score_predict_data   \n",
       "9             score_py   \n",
       "10        score_syntax   \n",
       "\n",
       "                                                                                                Test name  \\\n",
       "0                                             Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set   \n",
       "1           Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher   \n",
       "2                             Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.   \n",
       "3                                                   Check that field MODEL_ARTIFACT_VERSION is set to 3.0   \n",
       "4   Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory   \n",
       "5                                                                      Check that load_model() is defined   \n",
       "6                                                                         Check that predict() is defined   \n",
       "7                        Check that all other arguments in predict() are optional and have default values   \n",
       "8                                     Check that the only required argument for predict() is named \"data\"   \n",
       "9       Check that the file \"score.py\" exists and is in the top level directory of the artifact directory   \n",
       "10                                                                         Check for Python syntax errors   \n",
       "\n",
       "    Result Message  \n",
       "0   Passed          \n",
       "1   Passed          \n",
       "2   Passed          \n",
       "3   Passed          \n",
       "4   Passed          \n",
       "5   Passed          \n",
       "6   Passed          \n",
       "7   Passed          \n",
       "8   Passed          \n",
       "9   Passed          \n",
       "10  Passed          "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all should be passed\n",
    "artifact.introspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4a6ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_22432/1712819302.py:2: DeprecationWarning: Method save is deprecated in 2.6.6 and will be removed in a future release. Use framework specific Model utility class for saving and deploying model. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  catalog_entry = artifact.save(display_name='clothing_detection_v1', description='clothing_detection_v1', timeout=600)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ocid1.datasciencemodel.oc1.eu-frankfurt-1.amaaaaaangencdyaydz62h4jukgvduwl4af4jq4xugagc23vgael7fnbl5nq'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model artifact to the model catalog. \n",
    "catalog_entry = artifact.save(display_name='clothing_detection_v1', description='clothing_detection_v1', timeout=600)\n",
    "catalog_entry.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32545b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5c003",
   "metadata": {},
   "source": [
    "# **8. Deploy the ML Model and test real-time inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b686c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8718673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdya4k5yytlulz2kvgvmjmo3qz3mmk4ed6zld5ksdi7upstq/predict\n"
     ]
    }
   ],
   "source": [
    "uri = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdya4k5yytlulz2kvgvmjmo3qz3mmk4ed6zld5ksdi7upstq/predict\"\n",
    "print(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77613fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'prediction': {'cloting_features': '[\"vest dress\"]'}}\n"
     ]
    }
   ],
   "source": [
    "config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])\n",
    "\n",
    "import json\n",
    "\n",
    "#POST request to the model\n",
    "response = requests.post(uri, json=payload_json, auth=auth)\n",
    "print(response)\n",
    "xx = (json.loads(response.content))\n",
    "print(xx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79876f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d374f47",
   "metadata": {},
   "source": [
    "## **Local UI for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db3e0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "import json\n",
    "\n",
    "config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])\n",
    "\n",
    "\n",
    "\n",
    "def full_function(input_image):\n",
    "    \n",
    "    input_path=input_image #is the path\n",
    "\n",
    "    payload_string = str()\n",
    "    full_payload_string = str()\n",
    "    \n",
    "    with open(input_path, \"rb\") as image2string:\n",
    "        converted_string = base64.b64encode(image2string.read()).decode('ascii')\n",
    "\n",
    "    payload1 = json.dumps(converted_string)\n",
    "    json_payload1 = json.loads(payload1)\n",
    "\n",
    "    payload_json = {'data':{'input_image': json_payload1}}\n",
    "\n",
    "    uri = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdya4k5yytlulz2kvgvmjmo3qz3mmk4ed6zld5ksdi7upstq/predict\"\n",
    "    \n",
    "    #POST request to the model\n",
    "    response = requests.post(uri, json=payload_json, auth=auth)\n",
    "    print(response)\n",
    "    xx = (json.loads(response.content))    \n",
    "\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7690c8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://4a39dadd65c17de38c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4a39dadd65c17de38c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/gradio/blocks.py\", line 2014, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_22432/2999098039.py\", line 15, in <cell line: 15>\n",
      "    gr.Interface(fn=full_function, inputs=input_image, outputs=xx, title=desc).launch(share=True, debug=True) #\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/gradio/blocks.py\", line 1926, in launch\n",
      "    self.block_thread()\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/gradio/blocks.py\", line 2017, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/gradio/networking.py\", line 43, in close\n",
      "    self.thread.join()\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "desc = \"Clothing Detection\"\n",
    "\n",
    "with gr.Blocks() as demo: \n",
    "     \n",
    "    input_image = gr.Image(label=\"source_city\", type='filepath')\n",
    "\n",
    "    xx = gr.Text(label='Detected clothes')\n",
    "\n",
    "\n",
    "    submit_btn = gr.Button(\"Run Analysis\")\n",
    "\n",
    "\n",
    "gr.Interface(fn=full_function, inputs=input_image, outputs=xx, title=desc).launch(share=True, debug=True) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6297e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3e08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f2680d",
   "metadata": {},
   "source": [
    "## **2. Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "\n",
    "\n",
    "### Functions and labels\n",
    "###\n",
    "\n",
    "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']\n",
    "\n",
    "def fix_channels(t):\n",
    "    \"\"\"\n",
    "    Some images may have 4 channels (transparent images) or just 1 channel (black and white images), in order to let the images have only 3 channels. I am going to remove the fourth channel in transparent images and stack the single channel in back and white images.\n",
    "    :param t: Tensor-like image\n",
    "    :return: Tensor-like image with three channels\n",
    "    \"\"\"\n",
    "    if len(t.shape) == 2:\n",
    "        return ToPILImage()(torch.stack([t for i in (0, 0, 0)]))\n",
    "    if t.shape[0] == 4:\n",
    "        return ToPILImage()(t[:3])\n",
    "    if t.shape[0] == 1:\n",
    "        return ToPILImage()(torch.stack([t[0] for i in (0, 0, 0)]))\n",
    "    return ToPILImage()(t)\n",
    "    \n",
    "def idx_to_text(i):\n",
    "    return cats[i]\n",
    "\n",
    "# Random colors used for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        ax.text(xmin, ymin, idx_to_text(cl), fontsize=10,\n",
    "                bbox=dict(facecolor=c, alpha=0.8))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.savefig(\"image.png\")\n",
    "    \n",
    "    \n",
    "def visualize_predictions(image, outputs, threshold=0.8):\n",
    "    # keep only predictions with confidence >= threshold\n",
    "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > threshold\n",
    "\n",
    "    # convert predicted boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
    "\n",
    "    # plot results\n",
    "    plot_results(image, probas[keep], bboxes_scaled)\n",
    "\n",
    "    \n",
    "#############################\n",
    "\n",
    "#define model\n",
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#load iamge\n",
    "image = Image.open(open('./img_1.jpg', \"rb\"))\n",
    "image = fix_channels(ToTensor()(image))\n",
    "image = image.resize((600, 800))\n",
    "\n",
    "#apply model\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610bbaaf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213477ab",
   "metadata": {},
   "source": [
    "# **GenAI - LangChain - FAISS as Vector db - Gradio appxx**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b42743",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fe2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a conda env. I used tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate tiktoken openai gradio torch accelerate safetensors sentence-transformers faiss-gpu bitsandbytes pypdf typing-extensions PyPDF2 \n",
    "!pip install tokenizers --upgrade\n",
    "!pip install transformers -U\n",
    "!pip install langchain -U\n",
    "!pip install gradio --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6141e2e",
   "metadata": {},
   "source": [
    "# **Run the below cell after installations. Click on the public URL button**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5ffc5",
   "metadata": {},
   "source": [
    "## Add your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b7483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compartment_ocid=\"ocid1.compartment.oc1..aaaaaaaae3n6r6hrjipbap2hojicrsvkzatrtlwvsyrpyjd7wjnw4za3m75q\"\n",
    "max_return_from_vector = 4\n",
    "CHUNK_SIZE = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966a0ae",
   "metadata": {},
   "source": [
    "## Run the below. Click on Public URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72d9b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://bef910c58f4ae1eaeb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bef910c58f4ae1eaeb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "done with upload_files\n",
      "/tmp/gradio/b7d0a1434d39bdbbe0fae9ece7d64c970e022683/example_strategy.pdf\n",
      "Start add text\n",
      "Start generate_bot_response\n",
      "start process_files\n",
      "<PyPDF2._reader.PdfReader object at 0x7f795c9f12b0>\n",
      "Start chuncking\n",
      "Store embeddings in FAISS\n",
      "Start load GenAI\n",
      "returning qa_chain_with_memotry\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot response is\n",
      "{'query': 'What is an autonomous database?', 'result': 'An autonomous database is a self-driving database system that requires minimal human labor for database management tasks like provisioning, security, monitoring, backups, recovery, troubleshooting, and tuning. It is designed to run on the Exadata platform and leverages various Exadata features for improved performance. The Oracle Autonomous Database, for example, is built on the Oracle Database and is designed to provide superior performance, reliability, and scalability while minimizing human error and the need for manual management. It offers a cloud-native approach to data management that makes it easier to migrate existing applications with little to no modification. \\n\\nWould you like to know more about autonomous databases? ', 'source_documents': [Document(page_content='Database costs less to run, performs better, is more available, and eliminates human error.  \\nSelf -Driving  \\nYou tell the Autonomous Database the service level to achieve, and it handles the rest. The Autonomous Database \\neliminates human labor to provision, secure, monitor, backup, recover, troubleshoot, and tune databases.  T his greatly \\nreduces database maintenance tasks, reducing costs and freeing scarce administrator resources to work on higher value \\ntasks.  \\nSince the Autonomous Database is based on the extremely feature rich and proven Oracle Database, on the Exadata \\nplatfor m, it is able to run both OLTP and analytic workloads up to 100X faster. It includes many performance enhancing \\nExadata features such as persistent memory, smart flash cache, automatic columnar format in flash cache, smart scan, \\nExafusion communication ove r the super -fast RDMA over Converged Ethernet (RoCE)  network, and automatic storage \\nindexes.'), Document(page_content='application and enabling much f aster innovation.  \\nThe Autonomous Database subscription includes many management, testing, and security capabilities that previously had \\nto be licensed separately, including:  \\n\\uf097 Data Encryption  \\n\\uf097 Diagnostics Pack  \\n\\uf097 Tuning Pack  \\n\\uf097 Real Application Testing  \\n\\uf097 Data Masking , Redaction and Subsetting  \\n\\uf097 Hybrid Columnar Compression  \\n\\uf097 Database Vault  \\n\\uf097 Database In -Memory (subset) – in Autonomous Data Warehouse  \\n\\uf097 Advanced Analytics (subset) - in Autonomous Data Warehouse  \\nTo implement full data management workflows, other clouds use a combination of multiple specialized databases such as a \\nqueuing database, OLTP database, JSON data store, reporting database, analytics database, etc.  Each database is \\nindependently developed  and therefore has its own data model, security model, execution model, monitoring model, tuning \\nmodel, consistency model, query language, analytics, etc.  Data needs to be transformed and copied between these'), Document(page_content='a different infrastructure.  \\nInstead, IT leaders need to strike at the heart of the matter – rethink data management fo r a cloud -native world. They need \\ndata management that can manage on its own the core activities of performance, security, and availability. This new \\napproach to data management has to make data appear as if it were stored in a shape and made available at a scale ideal for \\nthe current workload, without having to predict it ahead of time. To deliver data management as a service like this, IT leade rs \\nneed a new kind of database.  \\n \\nTHE ORACLE AUTONOMOUS DATABASE  \\n \\nLike an autonomous car, the Oracle Autonomous Database (Autonomous Database) provides a level of performance and \\nreliability manually managed databases can’t deliver. Compared to a manually managed database, the Autonomous \\nDatabase costs less to run, performs better, is more available, and eliminates human error.  \\nSelf -Driving'), Document(page_content='Autonomous Database and Exadata Cloud@Customer  which provides an Autonomous Database slice of the Oracle \\nCloud both  running inside the customer’s data center.  \\n\\uf097 Easily go cloud -native with existing apps. Because the Autonomous Datab ase is still an Oracle database, existing apps \\ncan be quickly and easily moved to this new cloud -native data management platform with little to no app changes.  \\nWith the Autonomous Database, major cost savings and agility improvements come quickly, not aft er years to decades of \\napplication rewrites.  \\n \\n \\nSAFEST TRANSITION TO THE CLOUD  \\nThe transition to the cloud must improve the availability of mission -critical workloads, not put them at risk.   \\nThe Autonomous Database is built on top of the most widely proven  and sophisticated database in the world: Oracle \\nDatabase. The Oracle Database is capable of running any type of workload in a highly secure, available, and scalable fashion.')]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "done with upload_files\n",
      "/tmp/gradio/b7d0a1434d39bdbbe0fae9ece7d64c970e022683/example_strategy.pdf\n",
      "Start add text\n",
      "Start generate_bot_response\n",
      "start process_files\n",
      "<PyPDF2._reader.PdfReader object at 0x7f795c95c970>\n",
      "Start chuncking\n",
      "Store embeddings in FAISS\n",
      "Start load GenAI\n",
      "returning qa_chain_with_memotry\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot response is\n",
      "{'query': 'Yes, please!', 'result': \"Absolutely! I can provide you with information on the Oracle Autonomous Database Strategy technical brief. This brief offers an overview of Oracle's Autonomous Database and how it redefines data management for organisations. \\n\\nThe executive summary of the brief provides a concise introduction to the challenges that IT leaders face in managing digital businesses and the need for a new approach to data management. It highlights how the Oracle Autonomous Database addresses these challenges with its self-driving, self-securing, and self-repairing capabilities. \\n\\nThe brief then discusses the importance of uptime and reliability for mission-critical systems in the cloud. It notes how downtime can result in financial and reputational damage, and emphasizes the need for IT leaders to ensure that their cloud architecture can handle various threats to availability. \\n\\nTo address these concerns, the Oracle Autonomous Database is positioned as a next-generation solution for mission-critical data management in the cloud. It emphasises the Autonomous Database's ability to simplify data science experimentation and instant low-code development through Oracle Application Express (APEX). \\n\\nPlease note that the brief contains a disclaimer that some features and functionality discussed may not be present in the current release of the services. \\n\\nWould you like me to go into more detail about any specific aspects of the Oracle Autonomous Database Strategy technical brief? \", 'source_documents': [Document(page_content='discussed may not appear in the current release of the services.  \\nThis  document  is for informational  purposes  only  and is intended  solely  to assist  you in planning  for the implementation  \\nand upgrade  of the product  features  described.  It is not a commitment  to deliver  any material,  code,  or functionality,  and \\nshould  not be relied  upon  in making  purchasing  decisions.  The development,  release,  and timing  of any features  or \\nfunctionality  described  in this document  remains  at the sole discretion  of Oracle.  \\n   \\n3 Technical Brief   |  Oracle Autonomous Database Strategy  \\n Copyright © 2023 , Oracle and/or its affiliates  |  Public  \\n  \\n \\nTABLE OF CONTENTS  \\nPurpose Statement  2 \\nDisclaimer  2 \\nExecutive Summary  4 \\nThe Near Impossi ble Task of Managing a Digital Business by Hand  4 \\nIT Leaders Need a New Approach to Data Management Itself  5 \\nThe Oracle Autonomous Database  5 \\nSelf-Driving  5 \\nSelf-Securing  5 \\nSelf-Repairing  5'), Document(page_content='\\uf097 Mission -critical systems in the cloud must deliver guaranteed uptime.  Downtime for critical applications causes \\nsevere financial and reputational damage.   IT leaders must be sure that the cloud architecture they adopt handles every \\nthreat to availability, from software and hardware failures and maintenance to natural disasters.  \\n  \\n5 Technical Brief   |  Oracle Autonomous Database Strategy  \\n Copyright © 2023 , Oracle and/or its affiliates  |  Public  \\n IT LEADERS NEED A NEW APPROACH TO DATA MANAGEMENT ITSELF  \\n \\nIncremental changes around the edges aren’t enough to answer these challenges. Even moving the entire enterprise \\ncomputing estate to the Cloud is only a partial solution because this just shifts many of the same management difficulties to  \\na different infrastructure.  \\nInstead, IT leaders need to strike at the heart of the matter – rethink data management fo r a cloud -native world. They need'), Document(page_content='Oracle Autonomous Database \\nStrategy  \\n \\nNext Generation Mission -Critical Data Management in the Cloud  \\nJune, 2023   \\nCopyright © 2023 , Oracle and/or its affiliates  \\nPublic  \\n \\n \\n \\n2 Technical Brief   |  Oracle Autonom ous Database Strategy  \\n Copyright © 2023 , Oracle and/or its affiliates  |  Public  \\n  \\nPURPOSE STATEMENT  \\nThis  document  provides  an overview  of Oracle Autonomo us Database, and how this new era in data management will \\nchange the way organizations deal with database and data management in general as an outcome of the availability of a \\nnew class of self -driving, self -securing and self -repairing databases.  \\n \\nDISCLAIMER  \\nThis document  describes Oracle Autonomous Database strategy and roadmap. Some of the features and functionality \\ndiscussed may not appear in the current release of the services.  \\nThis  document  is for informational  purposes  only  and is intended  solely  to assist  you in planning  for the implementation'), Document(page_content='application code required to implement co mplex business logic.  \\n\\uf097 Instant Low -Code development environment. Oracle Application Express (APEX), a low code development framework \\nis pre -configured, and available as soon as an Autonomous Database is provisioned.  This easy -to-use development \\nenvironment  enables developers to quickly load data, manage database objects, develop REST interfaces, and build \\napps which look and run great on both desktop and mobile devices. Much of the development process is managed \\nautomatically including security, authenticat ion, database interactions, input validation, session state management and \\nmany other dependencies work out of the box.  \\n\\uf097 Simplify data science experimentation . Data science, like all science, boils down to experimentation. The Autonomous \\nDatabase’s built -in machine learning capabilities make it easy for data science teams to experiment with datasets that')]}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import langchain_community\n",
    "import langchain\n",
    "import oci\n",
    "import gradio as gr\n",
    "import torch\n",
    "import PyPDF2 # pdf reader\n",
    "import time\n",
    "import oci\n",
    "import ads\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import FAISS \n",
    "from langchain.chains import RetrievalQA \n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain.document_loaders import PyPDFDirectoryLoader \n",
    "from transformers import AutoTokenizer\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.llms import OCIGenAI\n",
    "import transformers\n",
    "import tokenizers\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Generate Embeddings\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "# Using HuggingFaceEmbeddings with the chosen embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",model_kwargs = {\"device\": \"cpu\"})                                                                         ### IF You use GPU, change 'cpu' to 'cuda'\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Load GenAI \n",
    "##########################################################################################################################################\n",
    "\n",
    "def load_llm():\n",
    "    \n",
    "    print(\"Start load GenAI\")\n",
    "    \n",
    "    compartment_id=compartment_ocid\n",
    "\n",
    "    llm = OCIGenAI(\n",
    "    model_id=\"cohere.command\",\n",
    "    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n",
    "    compartment_id=compartment_id,\n",
    "    model_kwargs = {\"max_tokens\":1024},\n",
    "    verbose=False)\n",
    "\n",
    "    return llm\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Create history\n",
    "##########################################################################################################################################\n",
    "\n",
    "def add_text(history, text):\n",
    "\n",
    "    print(\"Start add text\")\n",
    "    if not text:\n",
    "        raise gr.Error('Enter text')\n",
    "    history = history + [(text, '')]\n",
    "    return history\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Upload files\n",
    "##########################################################################################################################################\n",
    "\n",
    "def upload_file(files):\n",
    "    print(type(files))\n",
    "    print(\"done with upload_files\")\n",
    "    \n",
    "    files = files[0].name\n",
    "    print(files)\n",
    "\n",
    "    return files\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Process files\n",
    "##########################################################################################################################################\n",
    "\n",
    "def process_file(files):\n",
    "\n",
    "    print(\"start process_files\")\n",
    "    \"\"\"Function reads each loaded file, and extracts text from each of their pages\n",
    "    The extracted text is store in the 'text variable which is the passed to the splitter\n",
    "    to make smaller chunks necessary for easier information retrieval and adhere to max-tokens(4096) of DeciLM-7B-instruct\"\"\"\n",
    "\n",
    "    pdf_text = \"\"\n",
    "    for file in files:\n",
    "        pdf = PyPDF2.PdfReader(file.name)\n",
    "        print(pdf)\n",
    "        for page in pdf.pages:\n",
    "            pdf_text += page.extract_text()\n",
    "\n",
    "\n",
    "    # split into smaller chunks\n",
    "    print(\"Start chuncking\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=200)\n",
    "    splits = text_splitter.create_documents([pdf_text])\n",
    "\n",
    "    # create a FAISS vector store db. Create embeddings and adding to faiss\n",
    "    print(\"Store embeddings in FAISS\")\n",
    "    vectorstore_db = FAISS.from_documents(splits, embeddings)\n",
    "    \n",
    "\n",
    "    #create a custom prompt\n",
    "    custom_prompt_template = \"\"\"You have been given the following documents to answer the user's question.\n",
    "    If you do not have information from the files given to answer the questions just say I don't have information from the given files to answer. Do not try to make up an answer.\n",
    "    Context: {context}\n",
    "    History: {history}\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template, input_variables=[\"question\", \"context\", \"history\"])\n",
    "\n",
    "    # set QA chain with memory\n",
    "    qa_chain_with_memory = RetrievalQA.from_chain_type(llm=load_llm(),\n",
    "                                                       chain_type='stuff',\n",
    "                                                       return_source_documents=True,\n",
    "                                                       retriever=vectorstore_db.as_retriever(search_kwargs={\"k\": max_return_from_vector}),\n",
    "                                                                           chain_type_kwargs={\"verbose\": False,\n",
    "                                                                          \"prompt\": prompt,\n",
    "                                                                          \"memory\": ConversationBufferMemory(\n",
    "                                                                              input_key=\"question\",\n",
    "                                                                              memory_key=\"history\",\n",
    "                                                                              return_messages=True) })\n",
    "    # get answers\n",
    "    \n",
    "    print(\"returning qa_chain_with_memotry\")\n",
    "    \n",
    "    return qa_chain_with_memory\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Main\n",
    "##########################################################################################################################################\n",
    "\n",
    "def generate_bot_response(history,query, btn):\n",
    "    \n",
    "    print(\"Start generate_bot_response\")\n",
    "    \n",
    "    qa_chain_with_memory = process_file(btn) # run the qa chain with files from upload\n",
    "    bot_response = qa_chain_with_memory({\"query\": query})\n",
    "    \n",
    "    print(\"--\" *50)\n",
    "    print(\"Bot response is\")\n",
    "    print(bot_response)\n",
    "    print(\"--\" *50)\n",
    " \n",
    "    for char in bot_response['result']:\n",
    "        history[-1][-1] += char\n",
    "        time.sleep(0.05)\n",
    "        yield history,''\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Gradio\n",
    "##########################################################################################################################################\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    css=\".contain { display: flex !important; flex-direction: column !important; }\"\n",
    "    \"#component-0, #component-3, #component-10, #component-8  { height: 100% !important; }\"\n",
    "    \"#chatbot { flex-grow: 1 !important; overflow: auto !important;}\"\n",
    "    \"#col { height: 100vh !important; }\"\n",
    "    with gr.Row():\n",
    "            with gr.Row():\n",
    "              # Chatbot interface\n",
    "              chatbot = gr.Chatbot(label=\"Oracle GenAI\",\n",
    "                                   value=[],\n",
    "                                   elem_id='chatbot',\n",
    "                                   render=True,\n",
    "                                    bubble_full_width=False)\n",
    "                \n",
    "                \n",
    "            with gr.Column():\n",
    "                # PDF upload button\n",
    "                btn = gr.UploadButton(\"📁 Upload a PDF(s)\",\n",
    "                                      file_types=[\".pdf\"],\n",
    "                                      file_count=\"multiple\")\n",
    "                with gr.Row():\n",
    "                  # Uploaded PDFs window\n",
    "                  files = gr.File(label=\"Your PDFs\")\n",
    "\n",
    "            \n",
    "\n",
    "    with gr.Column():\n",
    "        with gr.Column():\n",
    "          # Ask question input field\n",
    "          txt = gr.Text(show_label=False, placeholder=\"Enter question\")\n",
    "\n",
    "        with gr.Column():\n",
    "          # button to submit question to the bot\n",
    "          submit_btn = gr.Button('Ask')\n",
    "\n",
    "    # Event handler for uploading a PDF\n",
    "    btn.upload(fn=upload_file, inputs=[btn], outputs=[files])\n",
    "\n",
    "    # Event handler for submitting text question and generating response\n",
    "    submit_btn.click(\n",
    "        fn= add_text,\n",
    "        inputs=[chatbot, txt],\n",
    "        outputs=[chatbot],\n",
    "        queue=True\n",
    "        ).success(\n",
    "          fn=generate_bot_response,\n",
    "          inputs=[chatbot, txt, btn],\n",
    "          outputs=[chatbot, txt]\n",
    "        ).success(\n",
    "          fn=upload_file,\n",
    "          inputs=[btn],\n",
    "          outputs=[files]\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     demo.queue()\n",
    "    demo.launch(share=True, debug=True) # launch app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9a3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1135e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
